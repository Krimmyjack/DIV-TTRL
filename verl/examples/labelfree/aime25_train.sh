#!/bin/bash
"""
AIME25 Training Script
Uses AI-MO/aimo-validation-aime as training set and math-ai/aime25 as test set.

Usage:
    # First, prepare the data:
    python examples/labelfree/prepare_aime25_data.py --local_dir ./data/AIME25-TTT
    cd data && python preprocess_simplerl.py && cd ..
    
    # Then run training:
    bash examples/labelfree/aime25_train.sh --backbone /path/to/Qwen3-4B-Base
    
Options:
    --backbone  Backbone model path (default: Qwen3-4B-Base)
    --clip-high[=VAL] Clip ratio control
    --ent       Entropy regularization coefficient (default: 0.000)
    --temp      Temperature parameter (default: 1.0)
"""

# === Environment Setup ===
unset VLLM_ATTENTION_BACKEND
export VLLM_USE_V1=1

# === Parse command line arguments ===
while [[ $# -gt 0 ]]; do
    case $1 in
        --backbone)
            BACKBONE="$2"
            shift 2
            ;;
        --clip-high)
            CLIP_HIGH="true"
            CLIP_SPECIFIED="true"
            CLIP_MODE="high"
            if [[ -n "$2" && "$2" != --* ]]; then
              CLIP_VALUE="$2"
              shift 2
            else
              shift 1
            fi
            ;;
        --clip-high=*)
            CLIP_HIGH="true"
            CLIP_SPECIFIED="true"
            CLIP_MODE="high"
            CLIP_VALUE="${1#--clip-high=}"
            shift 1
            ;;
        --ent)
            if [[ -z "$2" || "$2" == --* ]]; then
              shift 1
            else
              ENT="$2"
              shift 2
            fi
            ;;
        --ent=*)
            ENT="${1#--ent=}"
            shift 1
            ;;
        --temp)
            TEMP="$2"
            shift 2
            ;;
        --temp=*)
            TEMP="${1#--temp=}"
            shift 1
            ;;
        -h|--help)
            echo "Usage: $0 [--backbone BACKBONE] [--clip-high[=VAL]] [--ent COEFF] [--temp TEMP]"
            echo "  --backbone  Backbone model path (default: Qwen3-4B-Base)"
            echo "  --clip-high[=VAL] set clip ratio: not specified=0.2; flag only=0.28; with value use that value"
            echo "  --ent       Entropy regularization coefficient (float), e.g. 0.000/0.001/0.005 (default: 0.000)"
            echo "  --temp      Temperature parameter (float), e.g. 0.6/0.8/1.0 (default: 1.0)"
            echo "  -h, --help  Show help information"
            exit 0
            ;;
        *)
            shift
            ;;
    esac
done

# === Set default values ===
TASK="AIME25-TTT"
BACKBONE=${BACKBONE:-"Qwen3-4B-Base"}
CLIP_HIGH=${CLIP_HIGH:-"true"}
CLIP_SPECIFIED=${CLIP_SPECIFIED:-"false"}
CLIP_VALUE=${CLIP_VALUE:-""}
CLIP_MODE=${CLIP_MODE:-""}
ENT=${ENT:-"0.000"}
TEMP=${TEMP:-"1.0"}

ENTROPY_COEFF=$ENT

# Clean up any existing processes
pkill -f "python.*main_ppo" || true
pkill -f "python.*main_dapo" || true
pkill -f "multiprocessing.spawn" || true
ray stop --force 2>/dev/null || true
sleep 2
echo "========================="

# === Configuration ===
DATE=$(date +%m%d)
TIME_TAG=$(date +%H%M%S)

ADVANTAGE="diversity_density_hybrid"

echo "=== AIME25 Training Configuration ==="
echo "Task: $TASK"
echo "Backbone model: $BACKBONE"
echo "Advantage estimator: $ADVANTAGE"
echo "====================================="

# Set K value and sequence length
K=4
MAX_PROMPT_LENGTH=1024
MAX_RESPONSE_LENGTH=$((1024 * $K))
MAX_TOKEN_LEN=$((MAX_PROMPT_LENGTH + MAX_RESPONSE_LENGTH))
MAX_TOKEN_LEN2=$((MAX_TOKEN_LEN * 2))
if [ "$K" -gt 13 ]; then
  N=4
else
  N=16
fi

# Training parameters
EPISODE=14
DATA_TRAIN_BATCH_SIZE=16
N_VOTES_PER_PROMPT=64
N_SAMPLES_PER_PROMPT=32
MINI_BATCH_SIZE=1
MICRO_BATCH_SIZE=2

DATA_LOCAL_DIR="data"

# Parse backbone model path
CHAT_TEMPLATE=""
if [[ "$BACKBONE" == *"/"* ]]; then
  BACKBONE_PATH="$BACKBONE"
  BACKBONE_NAME="${BACKBONE##*/}"
else
  BACKBONE_PATH="/root/autodl-tmp/model/${BACKBONE}"
  BACKBONE_NAME="$BACKBONE"
fi

echo "Parsed model path: $BACKBONE_PATH"
echo "Parsed model name: $BACKBONE_NAME"

MODEL="${TASK}-${BACKBONE_NAME}"

EXPERIMENT="diversity-RL-AIME25"

# Set clip_ratio_high
if [ "$CLIP_SPECIFIED" = "true" ]; then
  if [ -n "$CLIP_VALUE" ]; then
    CLIP_RATIO_HIGH=$CLIP_VALUE
  else
    CLIP_RATIO_HIGH=0.28
  fi
  if [ "$CLIP_HIGH" = "true" ]; then
    EXPERIMENT="${EXPERIMENT}-ClipHigh"
  fi
else
  CLIP_RATIO_HIGH=0.2
fi

# Data files
TRAIN_FILES="train-simplerl.parquet"
TEST_FILES="test-simplerl.parquet"

# WandB configuration
WANDB_PROJECT="TTRL-AIME25"

if [ "$CLIP_HIGH" = "true" ]; then
  EXPERIMENT="${EXPERIMENT}-ClipHigh"
fi
EXPERIMENT="${EXPERIMENT}-Ent${ENTROPY_COEFF}"

LOG_NAME="${EXPERIMENT}-${MODEL}"
OUTPUT_DIR="checkpoints/${WANDB_PROJECT}/${MODEL}/${EXPERIMENT}"

echo "=== AIME25 TTRL Training Configuration ==="
echo "Task: $TASK"
echo "Backbone model: $BACKBONE"
echo "Advantage estimator: $ADVANTAGE"
if [[ "$ENTROPY_COEFF" != "0" && "$ENTROPY_COEFF" != "0.0" && "$ENTROPY_COEFF" != "0.00" && "$ENTROPY_COEFF" != "0.000" ]]; then
  ENT_ENABLED="true"
else
  ENT_ENABLED="false"
fi
echo "Enable entropy regularization: $ENT_ENABLED"
echo "Entropy coefficient: $ENTROPY_COEFF"
echo "Output directory: $OUTPUT_DIR"
echo "Experiment name: $LOG_NAME"
echo "==========================================="

# === Run Training ===
python -m verl.trainer.main_ppo \
  reward_model.reward_manager=diversity_ttrl \
  reward_model.reward_kwargs.n_samples_per_prompt=$N_SAMPLES_PER_PROMPT \
  reward_model.reward_kwargs.n_votes_per_prompt=$N_VOTES_PER_PROMPT \
  reward_model.reward_kwargs.mode="train" \
  data.train_files=["$DATA_LOCAL_DIR/$TASK/$TRAIN_FILES"] \
  data.val_files=["$DATA_LOCAL_DIR/$TASK/$TEST_FILES"] \
  data.max_prompt_length=$MAX_PROMPT_LENGTH \
  data.max_response_length=$MAX_RESPONSE_LENGTH \
  data.train_batch_size=$DATA_TRAIN_BATCH_SIZE \
  data.filter_overlong_prompts=True \
  data.truncation='error' \
  actor_rollout_ref.model.path=$BACKBONE_PATH \
  actor_rollout_ref.model.enable_gradient_checkpointing=True \
  actor_rollout_ref.model.use_remove_padding=True \
  actor_rollout_ref.actor.ppo_mini_batch_size=$MINI_BATCH_SIZE \
  actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=$MICRO_BATCH_SIZE \
  actor_rollout_ref.actor.use_kl_loss=True \
  actor_rollout_ref.actor.kl_loss_coef=0.001 \
  actor_rollout_ref.actor.clip_ratio_high=$CLIP_RATIO_HIGH \
  actor_rollout_ref.actor.optim.lr=5e-7 \
  actor_rollout_ref.actor.entropy_coeff=$ENTROPY_COEFF \
  actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=0.03 \
  actor_rollout_ref.actor.optim.warmup_style='cosine' \
  actor_rollout_ref.actor.fsdp_config.param_offload=False \
  actor_rollout_ref.actor.fsdp_config.optimizer_offload=False \
  actor_rollout_ref.actor.ppo_max_token_len_per_gpu=$((MAX_TOKEN_LEN2)) \
  actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=$MICRO_BATCH_SIZE \
  actor_rollout_ref.ref.fsdp_config.param_offload=True \
  actor_rollout_ref.rollout.name=vllm \
  actor_rollout_ref.rollout.temperature=$TEMP \
  actor_rollout_ref.rollout.enforce_eager=False \
  actor_rollout_ref.rollout.free_cache_engine=False \
  actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=$MICRO_BATCH_SIZE \
  actor_rollout_ref.rollout.tensor_model_parallel_size=1 \
  actor_rollout_ref.rollout.gpu_memory_utilization=0.5 \
  actor_rollout_ref.rollout.do_vote=True \
  actor_rollout_ref.rollout.n_vote=$N_VOTES_PER_PROMPT \
  actor_rollout_ref.rollout.n=$N_SAMPLES_PER_PROMPT \
  actor_rollout_ref.rollout.val_kwargs.do_sample=False \
  actor_rollout_ref.rollout.val_kwargs.top_p=0 \
  actor_rollout_ref.rollout.val_kwargs.temperature=0 \
  actor_rollout_ref.rollout.max_model_len=$((MAX_TOKEN_LEN)) \
  actor_rollout_ref.rollout.max_num_batched_tokens=$((MAX_TOKEN_LEN2)) \
  critic.optim.lr=9e-6 \
  critic.model.use_remove_padding=True \
  critic.model.path=$BACKBONE_PATH \
  critic.model.enable_gradient_checkpointing=True \
  critic.ppo_micro_batch_size_per_gpu=$MICRO_BATCH_SIZE \
  critic.model.fsdp_config.param_offload=False \
  critic.model.fsdp_config.optimizer_offload=False \
  algorithm.kl_ctrl.kl_coef=0.00 \
  algorithm.adv_estimator=$ADVANTAGE \
  trainer.logger=['console','wandb'] \
  trainer.project_name=$WANDB_PROJECT \
  trainer.experiment_name=$LOG_NAME \
  trainer.n_gpus_per_node=8 \
  trainer.nnodes=1 \
  trainer.save_freq=15 \
  trainer.test_freq=5 \
  trainer.max_actor_ckpt_to_keep=0 \
  trainer.max_critic_ckpt_to_keep=0 \
  trainer.default_local_dir=$OUTPUT_DIR \
  trainer.total_epochs=$EPISODE "$@"

echo "=== Training Completed ==="
echo "Output directory: $OUTPUT_DIR"
echo "Project name: $WANDB_PROJECT"
echo "Experiment name: $LOG_NAME"
